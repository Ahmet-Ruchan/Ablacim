"""
============================================
YASAA VISION - Scanned PDF Ingest Pipeline
============================================
Bu dosya TARANMIÅ (scanned) PDF'leri iÅŸler.

Normal PDF'lerden farkÄ±:
- page.get_text() Ã§alÄ±ÅŸmaz (metin yok, sadece resim var)
- Her sayfa resme Ã§evrilir (render)
- GPT-4o Vision ile metin Ã§Ä±karÄ±lÄ±r (OCR + Analiz)
- SonuÃ§ embedding'e Ã§evrilip MongoDB'ye kaydedilir

KullanÄ±m:
    python -m App.ingest.ingest_scanned

Yazar: Ahmet RuÃ§han
Tarih: 2024
============================================
"""

import os
import sys
import base64
import logging
from pathlib import Path
from typing import Optional, List, Dict, Any
from datetime import datetime

import fitz  # PyMuPDF
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_mongodb import MongoDBAtlasVectorSearch
from langchain_core.documents import Document
from pymongo import MongoClient

# ============================================
# LOGGING
# ============================================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# ============================================
# ENVIRONMENT
# ============================================
load_dotenv()

OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "")
MONGO_URI: str = os.getenv("MONGO_URI", "")
DB_NAME: str = os.getenv("DB_NAME", "YasaaVisionDB")
COLLECTION_NAME: str = os.getenv("COLLECTION_NAME", "palmistry_knowledge")
INDEX_NAME: str = os.getenv("INDEX_NAME", "vector_index")

# Scanned PDF klasÃ¶rÃ¼ (ayrÄ± tutuyoruz)
SCANNED_PDF_FOLDER: str = os.getenv("SCANNED_PDF_FOLDER", "App/pdf_storage/scanned")

# Vision ayarlarÄ±
VISION_MODEL: str = os.getenv("VISION_MODEL", "gpt-4o")
VISION_MAX_TOKENS: int = int(os.getenv("VISION_MAX_TOKENS", "2000"))

# Render ayarlarÄ±
RENDER_ZOOM: float = float(os.getenv("RENDER_ZOOM", "2.0"))  # 2x zoom = daha net gÃ¶rÃ¼ntÃ¼

# ============================================
# VISION PROMPT (TaranmÄ±ÅŸ Sayfa Ä°Ã§in)
# ============================================
SCANNED_PAGE_PROMPT = """
You are an expert OCR system specialized in palmistry books.

Analyze this scanned book page and extract ALL content:

1. **TEXT EXTRACTION:**
   - Extract ALL readable text from the page
   - Preserve paragraph structure
   - Include headings, subheadings, and captions
   - Transcribe any handwritten notes if visible

2. **DIAGRAM/ILLUSTRATION ANALYSIS:**
   - If there are hand diagrams, describe them in detail
   - Identify and name any palm lines shown (Heart Line, Head Line, Life Line, Fate Line, etc.)
   - Describe mounts, fingers, and special markings
   - Note any numbered labels or annotations on diagrams

3. **OUTPUT FORMAT:**
   - Write in clear, structured paragraphs
   - Use [DIAGRAM: ...] tags for illustration descriptions
   - Use [FIGURE X: ...] for numbered figures
   - Preserve the logical flow of the original page

IMPORTANT: 
- This is a SCANNED page, so quality may vary
- Extract EVERYTHING you can read
- If text is unclear, make your best interpretation and note [unclear]
- Output should be in the SAME LANGUAGE as the source (Turkish or English)
"""


# ============================================
# HELPER FUNCTIONS
# ============================================
def get_mongo_collection():
    """MongoDB koleksiyonuna baÄŸlanÄ±r."""
    client = MongoClient(MONGO_URI)
    db = client[DB_NAME]
    return db[COLLECTION_NAME]


def get_vector_store() -> MongoDBAtlasVectorSearch:
    """MongoDB Vector Store'u dÃ¶ndÃ¼rÃ¼r."""
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        openai_api_key=OPENAI_API_KEY
    )

    client = MongoClient(MONGO_URI)
    collection = client[DB_NAME][COLLECTION_NAME]

    vector_store = MongoDBAtlasVectorSearch(
        collection=collection,
        embedding=embeddings,
        index_name=INDEX_NAME,
        text_key="text",
        embedding_key="embedding"
    )

    return vector_store


def render_page_to_image(page: fitz.Page, zoom: float = 2.0) -> bytes:
    """
    PDF sayfasÄ±nÄ± PNG resme Ã§evirir.

    Args:
        page: PyMuPDF sayfa objesi
        zoom: BÃ¼yÃ¼tme faktÃ¶rÃ¼ (2.0 = 2x Ã§Ã¶zÃ¼nÃ¼rlÃ¼k)

    Returns:
        bytes: PNG formatÄ±nda resim
    """
    matrix = fitz.Matrix(zoom, zoom)
    pixmap = page.get_pixmap(matrix=matrix)
    return pixmap.tobytes("png")


def analyze_page_with_vision(llm: ChatOpenAI, image_bytes: bytes) -> str:
    """
    TaranmÄ±ÅŸ sayfa resmini GPT-4o Vision ile analiz eder.

    Args:
        llm: ChatOpenAI instance
        image_bytes: PNG formatÄ±nda sayfa resmi

    Returns:
        str: Sayfadan Ã§Ä±karÄ±lan metin ve aÃ§Ä±klamalar
    """
    # Base64'e Ã§evir
    image_base64 = base64.b64encode(image_bytes).decode("utf-8")

    # Vision API Ã§aÄŸrÄ±sÄ±
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": SCANNED_PAGE_PROMPT},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{image_base64}",
                        "detail": "high"  # YÃ¼ksek detay modu
                    }
                }
            ]
        }
    ]

    response = llm.invoke(messages)
    return response.content


def process_scanned_pdf(
        pdf_path: Path,
        llm: ChatOpenAI,
        vector_store: MongoDBAtlasVectorSearch
) -> Dict[str, Any]:
    """
    Tek bir taranmÄ±ÅŸ PDF'i iÅŸler.

    Args:
        pdf_path: PDF dosya yolu
        llm: ChatOpenAI instance
        vector_store: MongoDB Vector Store

    Returns:
        Dict: Ä°ÅŸlem istatistikleri
    """
    stats = {
        "file_name": pdf_path.name,
        "total_pages": 0,
        "processed_pages": 0,
        "failed_pages": 0,
        "documents_added": 0
    }

    logger.info(f"ğŸ“– PDF aÃ§Ä±lÄ±yor: {pdf_path.name}")

    try:
        doc = fitz.open(str(pdf_path))
        stats["total_pages"] = len(doc)
        logger.info(f"   ğŸ“„ Toplam sayfa: {len(doc)}")

        documents_to_add: List[Document] = []

        for page_num in range(len(doc)):
            page = doc[page_num]
            real_page_num = page_num + 1

            logger.info(f"   ğŸ”„ Sayfa {real_page_num}/{len(doc)} iÅŸleniyor...")

            try:
                # 1. SayfayÄ± resme Ã§evir
                image_bytes = render_page_to_image(page, zoom=RENDER_ZOOM)
                logger.info(f"      ğŸ“¸ Sayfa render edildi ({len(image_bytes)} bytes)")

                # 2. GPT-4o Vision ile analiz et
                extracted_text = analyze_page_with_vision(llm, image_bytes)
                logger.info(f"      ğŸ” Vision analizi tamamlandÄ± ({len(extracted_text)} karakter)")

                # 3. BoÅŸ kontrolÃ¼
                if len(extracted_text.strip()) < 50:
                    logger.warning(f"      âš ï¸ Sayfa {real_page_num} Ã§ok az iÃ§erik, atlanÄ±yor...")
                    continue

                # 4. Document oluÅŸtur
                metadata = {
                    "source": pdf_path.name,
                    "page": real_page_num,
                    "type": "scanned_book_page",
                    "processed_at": datetime.now().isoformat(),
                    "vision_model": VISION_MODEL
                }

                document = Document(
                    page_content=extracted_text,
                    metadata=metadata
                )

                documents_to_add.append(document)
                stats["processed_pages"] += 1

                logger.info(f"      âœ… Sayfa {real_page_num} baÅŸarÄ±yla iÅŸlendi")

            except Exception as e:
                logger.error(f"      âŒ Sayfa {real_page_num} hatasÄ±: {e}")
                stats["failed_pages"] += 1
                continue

        # 5. MongoDB'ye toplu kaydet
        if documents_to_add:
            logger.info(f"   ğŸ’¾ {len(documents_to_add)} dÃ¶kÃ¼man MongoDB'ye kaydediliyor...")
            vector_store.add_documents(documents_to_add)
            stats["documents_added"] = len(documents_to_add)
            logger.info(f"   âœ… KayÄ±t tamamlandÄ±!")

        doc.close()

    except Exception as e:
        logger.error(f"âŒ PDF iÅŸleme hatasÄ±: {e}")
        raise

    return stats


def find_scanned_pdfs(folder_path: str) -> List[Path]:
    """KlasÃ¶rdeki PDF dosyalarÄ±nÄ± bulur."""
    folder = Path(folder_path)

    if not folder.exists():
        logger.warning(f"âš ï¸ KlasÃ¶r bulunamadÄ±, oluÅŸturuluyor: {folder_path}")
        folder.mkdir(parents=True, exist_ok=True)
        return []

    pdf_files = list(folder.glob("*.pdf"))
    return pdf_files


# ============================================
# MAIN
# ============================================
def main():
    """Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu."""

    logger.info("=" * 60)
    logger.info("ğŸ”® YASAA VISION - Scanned PDF Ingest Pipeline")
    logger.info("=" * 60)

    # Kontroller
    if not OPENAI_API_KEY:
        logger.error("âŒ OPENAI_API_KEY bulunamadÄ±!")
        sys.exit(1)

    if not MONGO_URI:
        logger.error("âŒ MONGO_URI bulunamadÄ±!")
        sys.exit(1)

    # PDF'leri bul
    pdf_files = find_scanned_pdfs(SCANNED_PDF_FOLDER)

    if not pdf_files:
        logger.warning(f"âš ï¸ {SCANNED_PDF_FOLDER} klasÃ¶rÃ¼nde PDF bulunamadÄ±!")
        logger.info(f"   TaranmÄ±ÅŸ PDF'lerinizi ÅŸu klasÃ¶re koyun: {SCANNED_PDF_FOLDER}")
        sys.exit(0)

    logger.info(f"ğŸ“š {len(pdf_files)} adet PDF bulundu")

    # LLM ve Vector Store oluÅŸtur
    llm = ChatOpenAI(
        model=VISION_MODEL,
        max_tokens=VISION_MAX_TOKENS,
        openai_api_key=OPENAI_API_KEY
    )

    vector_store = get_vector_store()

    # Her PDF'i iÅŸle
    all_stats = []

    for pdf_path in pdf_files:
        logger.info("-" * 40)
        try:
            stats = process_scanned_pdf(pdf_path, llm, vector_store)
            all_stats.append(stats)
        except Exception as e:
            logger.error(f"âŒ {pdf_path.name} iÅŸlenemedi: {e}")
            all_stats.append({
                "file_name": pdf_path.name,
                "error": str(e)
            })

    # Ã–zet
    logger.info("=" * 60)
    logger.info("ğŸ“Š Ä°ÅLEM Ã–ZETÄ°")
    logger.info("=" * 60)

    total_pages = 0
    total_processed = 0
    total_failed = 0
    total_docs = 0

    for stat in all_stats:
        if "error" in stat:
            logger.error(f"   âŒ {stat['file_name']}: HATA - {stat['error']}")
        else:
            logger.info(f"   âœ… {stat['file_name']}:")
            logger.info(
                f"      Sayfa: {stat['total_pages']} | Ä°ÅŸlenen: {stat['processed_pages']} | Hata: {stat['failed_pages']}")
            total_pages += stat["total_pages"]
            total_processed += stat["processed_pages"]
            total_failed += stat["failed_pages"]
            total_docs += stat["documents_added"]

    logger.info("-" * 40)
    logger.info(
        f"ğŸ“ˆ TOPLAM: {total_pages} sayfa | {total_processed} iÅŸlendi | {total_failed} hata | {total_docs} dÃ¶kÃ¼man")
    logger.info("=" * 60)
    logger.info("âœ… Scanned PDF Ingest tamamlandÄ±!")


if __name__ == "__main__":
    main()
