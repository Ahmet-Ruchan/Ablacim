import streamlit as st
from dotenv import load_dotenv
from pypdf import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
import os
from pathlib import Path

load_dotenv()

# PDF klasÃ¶rÃ¼ - buraya PDF'lerini koy
PDF_FOLDER = "./docs"


@st.cache_resource
def load_and_process_pdfs():
    """BaÅŸlangÄ±Ã§ta tÃ¼m PDF'leri yÃ¼kle ve iÅŸle"""

    # KlasÃ¶r yoksa oluÅŸtur
    Path(PDF_FOLDER).mkdir(exist_ok=True)

    # PDF dosyalarÄ±nÄ± bul
    pdf_files = list(Path(PDF_FOLDER).glob("*.pdf"))

    if not pdf_files:
        return None, "docs klasÃ¶rÃ¼nde PDF bulunamadÄ±!"

    # Metinleri Ã§Ä±kar
    text = ""
    for pdf_path in pdf_files:
        reader = PdfReader(str(pdf_path))
        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"

    if not text.strip():
        return None, "PDF'lerden metin Ã§Ä±karÄ±lamadÄ±!"

    # Chunk'la
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    chunks = splitter.split_text(text)

    # VektÃ¶r DB oluÅŸtur
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma.from_texts(
        texts=chunks,
        embedding=embeddings,
        persist_directory="./chroma_db"
    )

    # Chain oluÅŸtur
    llm = ChatOpenAI(model="gpt-5.1", temperature=0)
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        memory=memory
    )

    return chain, f"âœ… {len(pdf_files)} PDF, {len(chunks)} chunk yÃ¼klendi"


def main():
    st.set_page_config(page_title="PDF RAG", page_icon="ğŸ“š")
    st.header("ğŸ“š PDF Chatbot")

    # PDF'leri yÃ¼kle (cache'lenir, sadece 1 kere Ã§alÄ±ÅŸÄ±r)
    chain, message = load_and_process_pdfs()

    st.sidebar.info(message)
    st.sidebar.markdown(f"**PDF KlasÃ¶rÃ¼:** `{PDF_FOLDER}`")

    if chain is None:
        st.warning(f"'{PDF_FOLDER}' klasÃ¶rÃ¼ne PDF dosyalarÄ±nÄ± koy ve sayfayÄ± yenile.")
        return

    # Chat geÃ§miÅŸi
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    # GeÃ§miÅŸi gÃ¶ster
    for msg in st.session_state.chat_history:
        with st.chat_message(msg["role"]):
            st.write(msg["content"])

    # Soru sor
    if question := st.chat_input("Soru sor..."):
        st.session_state.chat_history.append({"role": "user", "content": question})

        with st.chat_message("user"):
            st.write(question)

        with st.chat_message("assistant"):
            with st.spinner("..."):
                response = chain({"question": question})
                answer = response["answer"]
                st.write(answer)

        st.session_state.chat_history.append({"role": "assistant", "content": answer})


if __name__ == "__main__":
    main()